# ===========================================
# Model Evaluation Workflow
# ===========================================
# Runs on: After successful tests
# Purpose: Evaluate model performance and fail if below threshold
# ===========================================

name: Model Evaluation

on:
  workflow_run:
    workflows: ["Tests & Linting"]
    types:
      - completed
    branches:
      - main
      - develop

env:
  # Performance thresholds (adjusted for app reviews dataset)
  MIN_ACCURACY: 0.65
  MIN_F1_SCORE: 0.65
  MIN_PRECISION: 0.65
  MIN_RECALL: 0.60

jobs:
  # ================================
  # Evaluate Model Performance
  # ================================
  evaluate:
    name: Evaluate Model
    runs-on: ubuntu-latest
    # Only run if tests passed
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download test dataset
        run: |
          mkdir -p data
          # Download or use cached test data
          echo "Preparing test dataset..."

      - name: Run model evaluation
        id: evaluation
        run: |
          python -m src.evaluate --output metrics.json
          
          # Extract metrics
          ACCURACY=$(python -c "import json; print(json.load(open('metrics.json'))['accuracy'])")
          F1_SCORE=$(python -c "import json; print(json.load(open('metrics.json'))['f1_score'])")
          PRECISION=$(python -c "import json; print(json.load(open('metrics.json'))['precision'])")
          RECALL=$(python -c "import json; print(json.load(open('metrics.json'))['recall'])")
          
          echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT
          echo "f1_score=$F1_SCORE" >> $GITHUB_OUTPUT
          echo "precision=$PRECISION" >> $GITHUB_OUTPUT
          echo "recall=$RECALL" >> $GITHUB_OUTPUT
          
          echo "## ðŸ“Š Model Metrics" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value | Threshold | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Accuracy | $ACCURACY | ${{ env.MIN_ACCURACY }} | $(python -c "print('âœ…' if $ACCURACY >= ${{ env.MIN_ACCURACY }} else 'âŒ')") |" >> $GITHUB_STEP_SUMMARY
          echo "| F1 Score | $F1_SCORE | ${{ env.MIN_F1_SCORE }} | $(python -c "print('âœ…' if $F1_SCORE >= ${{ env.MIN_F1_SCORE }} else 'âŒ')") |" >> $GITHUB_STEP_SUMMARY
          echo "| Precision | $PRECISION | ${{ env.MIN_PRECISION }} | $(python -c "print('âœ…' if $PRECISION >= ${{ env.MIN_PRECISION }} else 'âŒ')") |" >> $GITHUB_STEP_SUMMARY
          echo "| Recall | $RECALL | ${{ env.MIN_RECALL }} | $(python -c "print('âœ…' if $RECALL >= ${{ env.MIN_RECALL }} else 'âŒ')") |" >> $GITHUB_STEP_SUMMARY

      - name: Upload metrics as artifact
        uses: actions/upload-artifact@v4
        with:
          name: model-metrics
          path: metrics.json
          retention-days: 30

      - name: Check performance thresholds
        run: |
          ACCURACY=${{ steps.evaluation.outputs.accuracy }}
          F1_SCORE=${{ steps.evaluation.outputs.f1_score }}
          PRECISION=${{ steps.evaluation.outputs.precision }}
          RECALL=${{ steps.evaluation.outputs.recall }}
          
          echo "Checking performance thresholds..."
          
          FAILED=0
          
          if (( $(echo "$ACCURACY < ${{ env.MIN_ACCURACY }}" | bc -l) )); then
            echo "âŒ Accuracy ($ACCURACY) is below threshold (${{ env.MIN_ACCURACY }})"
            FAILED=1
          fi
          
          if (( $(echo "$F1_SCORE < ${{ env.MIN_F1_SCORE }}" | bc -l) )); then
            echo "âŒ F1 Score ($F1_SCORE) is below threshold (${{ env.MIN_F1_SCORE }})"
            FAILED=1
          fi
          
          if (( $(echo "$PRECISION < ${{ env.MIN_PRECISION }}" | bc -l) )); then
            echo "âŒ Precision ($PRECISION) is below threshold (${{ env.MIN_PRECISION }})"
            FAILED=1
          fi
          
          if (( $(echo "$RECALL < ${{ env.MIN_RECALL }}" | bc -l) )); then
            echo "âŒ Recall ($RECALL) is below threshold (${{ env.MIN_RECALL }})"
            FAILED=1
          fi
          
          if [ $FAILED -eq 1 ]; then
            echo "::error::Model performance is below required thresholds!"
            exit 1
          fi
          
          echo "âœ… All performance metrics meet the required thresholds!"

      - name: Generate evaluation report
        if: always()
        run: |
          echo "# Model Evaluation Report" > evaluation_report.md
          echo "Generated on: $(date)" >> evaluation_report.md
          echo "" >> evaluation_report.md
          echo "## Metrics" >> evaluation_report.md
          cat metrics.json | python -m json.tool >> evaluation_report.md

      - name: Upload evaluation report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-report
          path: evaluation_report.md
          retention-days: 30
