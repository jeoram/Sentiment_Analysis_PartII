# ===========================================
# Model Evaluation Workflow
# ===========================================
name: Model Evaluation

on:
  workflow_run:
    workflows: ["Tests & Linting"]
    types:
      - completed

env:
  MIN_ACCURACY: 0.50

jobs:
  evaluate:
    name: Evaluate Model
    runs-on: ubuntu-latest
    
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install textblob scikit-learn pandas numpy joblib

      - name: Download NLTK data
        run: |
          python -c "import nltk; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger')"

      - name: Run evaluation
        run: |
          python << 'EOF'
          import json
          from textblob import TextBlob

          # Test data
          TEST_DATA = [
              ("I love this product! Amazing!", "positive"),
              ("Best ever! Highly recommend!", "positive"),
              ("Great quality!", "positive"),
              ("Wonderful!", "positive"),
              ("I hate this, terrible!", "negative"),
              ("Worst ever!", "negative"),
              ("Very disappointed!", "negative"),
              ("Awful product!", "negative"),
              ("It's okay", "neutral"),
              ("Average", "neutral"),
              ("Not bad", "neutral"),
              ("Fine", "neutral"),
          ]

          def predict(text):
              blob = TextBlob(text)
              polarity = blob.sentiment.polarity
              if polarity > 0.1:
                  return "positive"
              elif polarity < -0.1:
                  return "negative"
              return "neutral"

          correct = 0
          for text, expected in TEST_DATA:
              pred = predict(text)
              if pred == expected:
                  correct += 1
              print(f"{text[:30]:30} | Expected: {expected:8} | Got: {pred:8} | {'✓' if pred == expected else '✗'}")

          accuracy = correct / len(TEST_DATA)
          print(f"\nAccuracy: {accuracy:.2%} ({correct}/{len(TEST_DATA)})")

          metrics = {"accuracy": round(accuracy, 4), "correct": correct, "total": len(TEST_DATA)}
          with open("metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)
          
          print(f"\n{'✅ PASSED' if accuracy >= 0.5 else '❌ FAILED'}")
          exit(0 if accuracy >= 0.5 else 1)
          EOF

      - name: Upload metrics
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: model-metrics
          path: metrics.json
