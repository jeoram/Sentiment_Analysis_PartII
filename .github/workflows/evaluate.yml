# ===========================================
# Model Evaluation Workflow
# ===========================================
# Runs on: After successful tests
# Purpose: Evaluate model performance and fail if below threshold
# ===========================================

name: Model Evaluation

on:
  push:
    branches:
      - main
      - master
  workflow_run:
    workflows: ["Tests & Linting"]
    types:
      - completed
    branches:
      - main
      - master
      - develop

env:
  # Performance thresholds (for sample test data)
  MIN_ACCURACY: 0.50
  MIN_F1_SCORE: 0.50

jobs:
  # ================================
  # Evaluate Model Performance
  # ================================
  evaluate:
    name: Evaluate Model
    runs-on: ubuntu-latest
    # Run on push OR when tests workflow completed successfully
    if: ${{ github.event_name == 'push' || github.event.workflow_run.conclusion == 'success' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run model evaluation
        id: evaluation
        run: |
          # Create a simple evaluation script that uses sample data
          python << 'EOF'
          import json
          from src.model import SentimentModel

          # Sample test data (since dataset is not in repo)
          TEST_DATA = [
              ("I love this product! It's amazing!", "positive"),
              ("This is the best thing ever!", "positive"),
              ("Great quality and fast delivery", "positive"),
              ("Absolutely wonderful experience", "positive"),
              ("I hate this, it's terrible", "negative"),
              ("Worst purchase I ever made", "negative"),
              ("Very disappointed with the quality", "negative"),
              ("This product is awful", "negative"),
              ("It's okay, nothing special", "neutral"),
              ("Average product, does the job", "neutral"),
              ("Not bad, not great either", "neutral"),
              ("It's fine I guess", "neutral"),
          ]

          # Load model
          model = SentimentModel()
          model.load()

          # Evaluate
          correct = 0
          total = len(TEST_DATA)
          
          for text, expected in TEST_DATA:
              result = model.predict(text)
              if result["sentiment"] == expected:
                  correct += 1
          
          accuracy = correct / total
          
          metrics = {
              "accuracy": round(accuracy, 4),
              "f1_score": round(accuracy, 4),  # Simplified
              "precision": round(accuracy, 4),
              "recall": round(accuracy, 4),
              "num_samples": total,
              "correct": correct
          }
          
          print(f"Accuracy: {accuracy:.2%} ({correct}/{total})")
          
          with open("metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)
          
          print("Metrics saved to metrics.json")
          EOF

      - name: Display metrics
        run: |
          echo "## ðŸ“Š Model Evaluation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat metrics.json >> $GITHUB_STEP_SUMMARY

      - name: Upload metrics as artifact
        uses: actions/upload-artifact@v4
        with:
          name: model-metrics
          path: metrics.json
          retention-days: 30

      - name: Check thresholds
        run: |
          ACCURACY=$(python -c "import json; print(json.load(open('metrics.json'))['accuracy'])")
          echo "Accuracy: $ACCURACY"
          
          if (( $(echo "$ACCURACY < ${{ env.MIN_ACCURACY }}" | bc -l) )); then
            echo "âŒ Accuracy below threshold!"
            exit 1
          fi
          
          echo "âœ… Model evaluation passed!"
